{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BigARTM CLI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Приложение `bigartm.exe`\n",
    "\n",
    "Приложение `bigartm.exe` (в Unix — просто `bigartm`) — самостоятельный бинарик, работающий по следующему алгоритму:\n",
    "\n",
    "1. Загрузить словарь, если он указан\n",
    "2. Инициализировать модель\n",
    "3. Если батчи еще не созданы — прочитать корпус в указанном формате и создать батчи\n",
    "4. Сделать определенное число итераций Online-EM алгоритма по указанным батчам, в соответствии с настройками\n",
    "5. При последнем проходе по корпусу, вывести в файл предсказанные тематические профили документов (вектора $\\theta$)\n",
    "6. Вывести модель/словарь в файлы\n",
    "\n",
    "#### Самостоятельный бинарник\n",
    "\n",
    "Бинарник не должен зависеть от динамически линкуемых библиотек. При переносе бинарника на другую машину он должен запускаться без дополнительных файлов и переменных окружения. Самостоятельность бинарика значительно упростит его использование на кластере для распределенного обучения модели и вывода тематических профилей документов. \n",
    "\n",
    "Для сборки самостоятельного бинарика `gcc` нужно передавать флаг `-static`. В `cmake` нужно прописать что-то такое:\n",
    "```\n",
    "SET(CMAKE_EXE_LINKER_FLAGS \"-static\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ключи консольного приложения\n",
    "\n",
    "- `--rand-seed <seed>`: инициализация генератора случайных чисел, гарантируется что приложение отрабатывает с одинаковым результатом при одинаковом значении seed\n",
    "\n",
    "### Dictionary\n",
    "\n",
    "Использование существующего словаря:\n",
    "- `--use-dictionary <dictionary-file>`: использовать словарь, загруженный из файла в бинарном формате\n",
    "- `--use-dictionary-bow <vocab-txt>`: использовать текстовый словарь в формате vocab.xxx.txt из UCI Bag-of-Words\n",
    "- настройки динамического словаря (в случае если он меняется), мин частота, макс число токенов\n",
    "\n",
    "Если не указана опция использования существующего словаря, то по-умолчанию создается новый словарь.\n",
    "\n",
    "Создание нового словаря по данным (токены добавляются автоматически как в [примере](https://github.com/bigartm/bigartm/blob/master/src/python/examples/example14_initialize_topic_model.py)):\n",
    "- `--dictionary-min-items <N/P>`: брать только слова, которые присутствуют не менее чем в N документах (если указано целое >= 1) или P% документов (если указано вещественное < 1)\n",
    "- `--dictionary-max-items <N/P>`: ... не более чем в N документах\n",
    "- `--dictionary-min-total-count <N>`: брать только слова, которые встретились не менее N раз во всей коллекции\n",
    "\n",
    "При построении словаря учитываются опции `use-modalities / ignore-modalities` (из Learning Options): словарь по неиспользуемым модальностям не составляется.\n",
    "\n",
    "### Corpus\n",
    "\n",
    "- `--corpus-format [vw,bow,mm]`: формат в котором потупает корпус текстов\n",
    "- `--read-corpus <file/url>`: источник, из которого поступает корпус в сыром виде: stdin / файл / каталог-с-файлами / сеть\n",
    "- `--read-batches <batches-path>`: путь к батчам, использовать заранее подготовленные батчи\n",
    "\n",
    "По умолчанию считается что подготовленные батчи отсутствуют и сырой корпус читается из stdin в формате vw.\n",
    "\n",
    "### Model\n",
    "\n",
    "- `--load-model <model-file>`: загрузить модель из файла\n",
    "\n",
    "Если не указана опция загрузки модели, то модель инициализируется.\n",
    "\n",
    "- `--initialize <scheme>`: создать новую модель, инициализировав ее одной из доступных схем \n",
    "- `--num-topics`: задать фиксированное число тем\n",
    "- `--topics <topics-file>`: указать файл с названиями топиков\n",
    "\n",
    "### Learning Options\n",
    "\n",
    "Базовые ключи:\n",
    "- `--passes <N>`: число проходов по корпусу, по умолчанию N=1\n",
    "\n",
    "Параметры обучения:\n",
    "- `--inner-iterations-count <N>`\n",
    "- `--decay-weight <w>`\n",
    "- `--apply-weight <w>`\n",
    "\n",
    "Модальности:\n",
    "- `--use-modalities <modalities>`: использовать только указанные модальности\n",
    "- `--ignore-modalities <modalities>`: отключить указанные модальности (используется одно из двух: use-modalities либо ignore-modalities)\n",
    "- `--modality-weights <weights>`: назначить веса различным модальностям\n",
    "- `--modality-weights word:1,author:10.0,link:0.2`\n",
    "\n",
    "Регуляризация:\n",
    "- регуляризатор задается набором `(тип, параметры, вес, модальности, топики)`\n",
    "- модальности указываются в формате `@name`\n",
    "- топики указываются в формате `#topic`\n",
    "- `--regularizer smoothPhi:0.1`\n",
    "- `--regularizer sparseTheta:25.0`\n",
    "- `--regularizer \"decorrelation:10 @word\"`\n",
    "- `--regularizer \"specific(x,y):0.1 #topic1:0.1,topic2:2.0 @word:0.5,author:0.1\"`\n",
    "\n",
    "**Use case**: шумовые темы\n",
    "\n",
    "**Идея**: конфигурационный файл с параметрами регуляризации вместо объемных параметров, хотя параметры можно тоже оставить. А может быть конфигурационный файл с вообще всеми параметрами обучения, аргументами задается только откуда вводить и куда выводить.\n",
    "\n",
    "### Multicore Mode\n",
    "\n",
    "- `--threads <N>`: число потоков в которых производить обучение, по умолчанию N=1\n",
    "\n",
    "### Distributed Mode\n",
    "\n",
    "*Пока не нужно реализовывать.*\n",
    "\n",
    "### Output\n",
    "\n",
    "- `--write-batches <batches-path>`: при первом чтении генерировать батчи и записывать по назначению\n",
    "- `--write-model <model-file>`: по окончанию обучения записать модель в файл\n",
    "- `--write-dictionary <dictionary-file>`: вывести словарь в файл\n",
    "- `--write-predictions <predictions-file>`: вывести тематические профили документов с последнего прохода по коллекции, в формате CSV. Возможность вывода в stdout!\n",
    "- `--write-model-readable <model-csv-file>`: по окончанию обучения записать модель в текстовый файл в формате CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Информационный вывод и логирование\n",
    "\n",
    "Информационный вывод должен производиться исключительно в stderr, поскольку в stdout может подаваться результат — тематические профили документов.\n",
    "\n",
    "Что выводить в stderr:\n",
    "- число проходов\n",
    "- число обработанных документов\n",
    "- метрики качества\n",
    "  - перплексия\n",
    "  - разреженность по модальностям\n",
    "- способы подсчета метрик: hold-out / [progressive-validation](http://hunch.net/~jl/projects/prediction_bounds/thesis/mathml/thesisse44.xml) \n",
    "- логарифмический вывод (как в vw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Примеры использования\n",
    "\n",
    "На этих примерах нужно протестировать новое CLI, а в конечном счете – сделать подробное описание этих примеров в документации.\n",
    "\n",
    "#### 1. Эксперимент на датасетах UCI Bag-of-Words (`kos`, `nytimes`)\n",
    "\n",
    "#### 2. Параллельное обучение модели мультиязычной Википедии\n",
    "\n",
    "#### 3. Использование тематической модели Википедии для категоризации страниц в интернете\n",
    "\n",
    "Продемонстрировать использование `bigartm` в Map-Reduce задаче при помощи стриминга."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пакетирование\n",
    "\n",
    "### Пакет для Debian\n",
    "\n",
    "В идеале BigARTM пользователь должен не собирать из исходников, а устанавливать вот таким образом:\n",
    "`apt-get install bigartm`\n",
    "\n",
    "Что нужно при этом установить:\n",
    "- SO: `/usr/lib/bigartm.so`\n",
    "- CLI: `/usr/bin/bigartm`\n",
    "- Python-пакет: `/usr/lib/python2.7/site-packages`\n",
    "\n",
    "[Руководство по созданию deb-пакетов](http://ubuntuforums.org/showthread.php?t=910717)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
