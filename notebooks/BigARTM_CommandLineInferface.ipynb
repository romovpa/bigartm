{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BigARTM CLI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Приложение `bigartm.exe`\n",
    "\n",
    "Приложение `bigartm.exe` (в Unix — просто `bigartm`) — самостоятельный бинарик, работающий по следующему алгоритму:\n",
    "\n",
    "1. Загрузить словарь, если он указан\n",
    "2. Инициализировать модель\n",
    "3. Если батчи еще не созданы — прочитать корпус в указанном формате и создать батчи\n",
    "4. Сделать определенное число итераций Online-EM алгоритма по указанным батчам, в соответствии с настройками\n",
    "5. При последнем проходе по корпусу, вывести в файл предсказанные тематические профили документов (вектора $\\theta$)\n",
    "6. Вывести модель/словарь в файлы\n",
    "\n",
    "#### Самостоятельный бинарник\n",
    "\n",
    "Бинарник не должен зависеть от динамически линкуемых библиотек. При переносе бинарника на другую машину он должен запускаться без дополнительных файлов и переменных окружения. Самостоятельность бинарика значительно упростит его использование на кластере для распределенного обучения модели и вывода тематических профилей документов. \n",
    "\n",
    "Для сборки самостоятельного бинарика `gcc` нужно передавать флаг `-static`. В `cmake` нужно прописать что-то такое:\n",
    "```\n",
    "SET(CMAKE_EXE_LINKER_FLAGS \"-static\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ключи консольного приложения\n",
    "\n",
    "- `--rand-seed <seed>`: инициализация генератора случайных чисел, гарантируется что приложение отрабатывает с одинаковым результатом при одинаковом значении seed. По-умолчанию выбирается по таймеру (случайно).\n",
    "\n",
    "### Dictionary\n",
    "\n",
    "Использование существующего словаря:\n",
    "- `--use-dictionary <dictionary-file>`: использовать словарь, загруженный из файла в бинарном формате\n",
    "- `--use-dictionary-bow <vocab-txt>`: использовать текстовый словарь в формате vocab.xxx.txt из UCI Bag-of-Words\n",
    "- настройки динамического словаря (в случае если он меняется), мин частота, макс число токенов\n",
    "\n",
    "Если не указана опция использования существующего словаря, то по-умолчанию создается новый словарь.\n",
    "\n",
    "Создание нового словаря по данным (токены добавляются автоматически как в [примере](https://github.com/bigartm/bigartm/blob/master/src/python/examples/example14_initialize_topic_model.py)):\n",
    "- `--dictionary-min-items <N/P>`: брать только слова, которые присутствуют не менее чем в N документах (если указано целое >= 1) или P% документов (если указано вещественное < 1)\n",
    "- `--dictionary-max-items <N/P>`: ... не более чем в N документах\n",
    "- `--dictionary-min-total-count <N>`: брать только слова, которые встретились не менее N раз во всей коллекции\n",
    "\n",
    "При построении словаря учитываются опции `use-modalities / ignore-modalities` (из Learning Options): словарь по неиспользуемым модальностям не составляется.\n",
    "\n",
    "### Corpus\n",
    "\n",
    "- `--corpus-format [vw,bow,mm]`: формат в котором потупает корпус текстов\n",
    "- `--read-corpus <file/url>`: источник, из которого поступает корпус в сыром виде: stdin / файл / каталог-с-файлами / сеть\n",
    "- `--read-batches <batches-path>`: путь к батчам, использовать заранее подготовленные батчи\n",
    "\n",
    "По умолчанию считается что подготовленные батчи отсутствуют и сырой корпус читается из stdin в формате vw.\n",
    "\n",
    "### Model\n",
    "\n",
    "- `--load-model <model-file>`: загрузить модель из файла\n",
    "\n",
    "Если не указана опция загрузки модели, то модель инициализируется.\n",
    "\n",
    "- `--initialize <scheme>`: создать новую модель, инициализировав ее одной из доступных схем \n",
    "- `--num-topics`: задать фиксированное число тем\n",
    "- `--topics <topics-file>`: указать файл с названиями топиков\n",
    "\n",
    "### Learning Options\n",
    "\n",
    "Базовые ключи:\n",
    "- `--passes <N>`: число проходов по корпусу, по умолчанию N=1\n",
    "\n",
    "Параметры обучения:\n",
    "- `--inner-iterations-count <N>`\n",
    "- `--decay-weight <w>`\n",
    "- `--apply-weight <w>`\n",
    "\n",
    "Модальности:\n",
    "- `--use-modalities <modalities>`: использовать только указанные модальности\n",
    "- `--ignore-modalities <modalities>`: отключить указанные модальности (используется одно из двух: use-modalities либо ignore-modalities)\n",
    "- `--modality-weights <weights>`: назначить веса различным модальностям\n",
    "- `--modality-weights word:1,author:10.0,link:0.2`\n",
    "\n",
    "Регуляризация:\n",
    "- регуляризатор задается набором `(вес, тип, параметры, модальности, топики)`\n",
    "- модальности указываются в формате `@name`\n",
    "- топики указываются в формате `#topic`\n",
    "- `--regularizer \"0.1 smoothPhi\"`\n",
    "- `--regularizer \"25.0 sparseTheta\"`\n",
    "- `--regularizer \"10 decorrelation @word\"`\n",
    "- `--regularizer \"0.1 specific(x,y) #topic1:0.1,topic2:2.0 @word:0.5,author:0.1\"`\n",
    "\n",
    "\n",
    "### Multicore Mode\n",
    "\n",
    "- `--threads <N>`: число потоков в которых производить обучение, по умолчанию N=1\n",
    "\n",
    "### Distributed Mode\n",
    "\n",
    "*Пока не нужно реализовывать.*\n",
    "\n",
    "### Output\n",
    "\n",
    "- `--write-batches <batches-path>`: при первом чтении генерировать батчи и записывать по назначению\n",
    "- `--write-model <model-file>`: по окончанию обучения записать модель в файл\n",
    "- `--write-dictionary <dictionary-file>`: вывести словарь в файл\n",
    "- `--write-predictions <predictions-file>`: вывести тематические профили документов с последнего прохода по коллекции, в формате CSV. Возможность вывода в stdout!\n",
    "- `--write-model-readable <model-csv-file>`: по окончанию обучения записать модель в текстовый файл в формате CSV\n",
    "\n",
    "- `--batch-size <N>`: число документов в одном батче"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Информационный вывод и логирование\n",
    "\n",
    "Информационный вывод должен производиться исключительно в stderr, поскольку в stdout может подаваться результат — тематические профили документов.\n",
    "\n",
    "Что выводить в stderr:\n",
    "- число проходов\n",
    "- число обработанных документов\n",
    "- метрики качества\n",
    "  - перплексия\n",
    "  - разреженность по модальностям\n",
    "- способы подсчета метрик: hold-out / [progressive-validation](http://hunch.net/~jl/projects/prediction_bounds/thesis/mathml/thesisse44.xml) \n",
    "- логарифмический вывод (как в vw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Форматы выходных файлов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Примеры использования\n",
    "\n",
    "На этих примерах нужно протестировать новое CLI, а в конечном счете – сделать подробное описание этих примеров в документации.\n",
    "\n",
    "### 1. Эксперимент на датасетах UCI Bag-of-Words\n",
    "\n",
    "Предположим, что мы загрузили [данные с UCI](https://archive.ics.uci.edu/ml/datasets/Bag+of+Words) в каталог `data`.\n",
    "\n",
    "Обучение ванильной модели PLSA\n",
    "```bash\n",
    "bigartm \\\n",
    "    --use-dictionary-bow data/vocab.enron.txt \\\n",
    "    --corpus-format bow \\\n",
    "    --read-corpus data/docword.enron.txt \\\n",
    "    --num-topics 50 \\\n",
    "    --passes 30 \\\n",
    "    --write-model enron_50topics.model \\\n",
    "    --write-predictions enron_50topics.theta.txt \\\n",
    "    --write-model-readable enron_50topics.phi.txt \n",
    "```\n",
    "\n",
    "Инициализируем модель для корпуса `nytimes` несколькими итерациями без регуляризаторов, создадим 100 тем с префиксом `topic` (целевые темы) и 3 темы `noise` (шумовые):\n",
    "```bash\n",
    "bigartm \\\n",
    "    --use-dictionary-bow data/vocab.nytimes.txt \\\n",
    "    --corpus-format bow \\\n",
    "    --read-corpus data/docword.nytimes.txt \\\n",
    "    --batch-size 1000 \\\n",
    "    --topics \"topic[100],noise[3]\" \\ # создает 103 темы: topic0,topic1,...,topic99,noise0,...,noise2\n",
    "    --passes 5 \\\n",
    "    --write-model nytimes_100topics_3noise.init.model \\\n",
    "    --write-batches nytimes.batches \\ # название каталога с батчами, который необходимо создать\n",
    "    --write-dictionary nytimes.dict \\ # словарь в бинарном формате\n",
    "    --threads 8  # первая итерация происходит последовательно, поскольку нет батчей\n",
    "                 # начиная со второй — батчи лежат в nytimes.batches, BigARTM начинает работать параллельно\n",
    "```\n",
    "\n",
    "Продолжим обучение модели. Теперь включаем регуляризаторы; шумовые темы будут сглаживаться, а целевые — разреживаться и декоррелироваться:\n",
    "```bash\n",
    "bigartm \\\n",
    "    --use-dictionary nytimes.dict \\\n",
    "    --read-batches nytimes.batches \\\n",
    "    --load-model nytimes_100topics_3noise.init.model \\\n",
    "    --regularizer \"0.5 smoothPhi #noise*\" \\\n",
    "    --regularizer \"0.5 sparsePhi #topic*\" \\\n",
    "    --regularizer \"0.5 smoothTheta #noise*\" \\\n",
    "    --regularizer \"0.5 sparseTheta #topic*\" \\\n",
    "    --regularizer \"1000 decorrelationPhi #topic*\" \\\n",
    "    --passes 50 \\\n",
    "    --decay-weight 0.9 \\  # детальная настройка итераций\n",
    "    --apply-weight 0.1 \\\n",
    "    --write-model nytimes_100topics_3noise.model \\\n",
    "    --write-predictions nytimes_100topics_3noise.theta.txt \\\n",
    "    --write-model-readable nytimes_100topics_3noise.phi.txt \\\n",
    "    --threads 32\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Обучение модели мультиязычной Википедии и использование для разметки документов\n",
    "\n",
    "По корпусу статей Википедии построим многоязычную тематическую модель, которую затем сможем использовать для категоризации произвольных страниц.\n",
    "\n",
    "Для обучения тематической модели был подготовлен мультиязычный корпуc в построчном формате vw, его положили в партицированном виде в HDFS (как результат работы MapReduce задачи, к примеру):\n",
    "```bash\n",
    "hdfs -cat wikipedia_corpus/part_00001\n",
    "\n",
    "  page_2342123 |en computer:2 device:3 mouse input |ru мышь:2 устройство компьютер |es ratón dispositivo:3 computadora:2\n",
    "  page_5645623 |en crusade:4 heretics:2 jerusalem |fr croisades:3 hérétique:2\n",
    "  ...\n",
    "```\n",
    "\n",
    "Инициализируем модель и словарь, корпус подается на вход через stdin:\n",
    "```bash\n",
    "hdfs -cat wikipedia_corpus/\\* | bigartm \\\n",
    "    --dictionary-min-items 3 \\   # брать слово если оно нашлось не менее чем в 3х документах\n",
    "    --dictionary-max-items 0.3 \\ #  ... не более чем в 30% документов\n",
    "    --corpus-format vw \\\n",
    "    --read-corpus - \\ # минус значит что читать из stdin (так принято в unix)\n",
    "    --topics topic[1000] \\\n",
    "    --use-modalities en,ru,fr,es,de,it \\\n",
    "    --write-batches wikipedia.batches \\\n",
    "    --write-dictionary wikipedia.dict \\\n",
    "    --write-model wikipedia_1k_topics.initial.model\n",
    "```\n",
    "\n",
    "Теперь обучим хорошую модель, параллельно:\n",
    "```bash\n",
    "bigartm \\\n",
    "    --read-batches wikipedia.batches \\\n",
    "    --load-model wikipedia_1k_topics.initial.model \\\n",
    "    --passes 100 \\\n",
    "    --regularizer \"0.5 sparsePhi #topic*\" \\\n",
    "    --decay-weight 0.95 \\\n",
    "    --write-model wikipedia_1k_topics.model \\\n",
    "    --write-model-readable wikipedia_1k_topics.phi.txt \\\n",
    "    --threads 32\n",
    "```\n",
    "\n",
    "Ура, теперь можем использовать модель для разметки документов по темам:\n",
    "```bash\n",
    "echo \"new_document |ru пхнглуи мглвнафх рльех вгахнагл фхтагн ктулху\" | bigartm  \\\n",
    "    --load-model wikipedia_1k_topics.model \\\n",
    "    --corpus-format vw \\\n",
    "    --read-corpus - \\     # прочитать корпус из stdin \n",
    "    --write-predictions - # записать предсказания в stdout\n",
    "    \n",
    "  new_document    0    0    0    0.3    0    0    0    ...    0    0.1    0\n",
    "```\n",
    "В таком виде BigARTM можно использовать для разметки произвольно большого набора документов при помощи MapReduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пакетирование\n",
    "\n",
    "### Пакет для Debian\n",
    "\n",
    "В идеале BigARTM пользователь должен не собирать из исходников, а устанавливать вот таким образом:\n",
    "`apt-get install bigartm`\n",
    "\n",
    "Что нужно при этом установить:\n",
    "- SO: `/usr/lib/bigartm.so`\n",
    "- CLI: `/usr/bin/bigartm`\n",
    "- Python-пакет: `/usr/lib/python2.7/site-packages`\n",
    "\n",
    "[Руководство по созданию deb-пакетов](http://ubuntuforums.org/showthread.php?t=910717)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
